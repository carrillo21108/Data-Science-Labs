{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Profunda (DNN) para clasificación MNIST\n",
    "\n",
    "Aplicaremos todos nuestros conocimientos para crear una DNN, frecuentemente llamada también una Artificial Neural Network (ANN).  El problema que vamos a trabajar se conoce como el \"Hola Mundo\" del aprendizaje profundo porque para la mayoría de estudiantes este es el primer algoritmo de aprendizaje profundo que ven. \n",
    "\n",
    "El conjunto de datos se llama MNIST y se refiere al reconocimiento de dígitos escritos a mano.  Pueden encontrar más información en el sitio web de Yann LeCun (Director of AI Research, Facebook).  El es uno de los pioneros de todo este tema, así como de otras metodologías más complejas como las Redes Neurales Convolucionales (CNN) que se utilizan hoy día.\n",
    "\n",
    "El conjunto de datos tiene 70,000 imágenes (28x28 pixels) de dígitos escritos a mano (1 dígito por imagen).\n",
    "\n",
    "La meta es escribir un algoritmo que detecta qué dígito ha sido escrito.  Como solo hay 10 dígitos (0 al 9), este es un problema de clasificación con 10 clases.\n",
    "\n",
    "Nuestra meta será construir una RN con 2 capas escondidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los paquetes relevantes\n",
    "\n",
    "TensorFlow incluye un proveedor de datos de MNIST que utilizaremos acá.  Viene con el módulo **\"tensorflow.keras.datasets\"**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting tensorflow-intel==2.17.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached h5py-3.11.0-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Collecting ml-dtypes<0.5.0,>=0.3.1 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.0-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\bcarr\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (71.0.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\bcarr\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached grpcio-1.65.4-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting tensorboard<2.18,>=2.17 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.2.0 (from tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached keras-3.4.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\python312\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.44.0)\n",
      "Collecting rich (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Collecting optree (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached optree-0.12.1-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2024.7.4)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bcarr\\appdata\\roaming\\python\\python312\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.17.0-cp312-cp312-win_amd64.whl (2.0 kB)\n",
      "Using cached tensorflow_intel-2.17.0-cp312-cp312-win_amd64.whl (385.2 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.65.4-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "Using cached h5py-3.11.0-cp312-cp312-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.4.1-py3-none-any.whl (1.1 MB)\n",
      "Using cached ml_dtypes-0.4.0-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Using cached protobuf-4.25.4-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "Using cached optree-0.12.1-cp312-cp312-win_amd64.whl (267 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow-intel, tensorflow\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] El sistema no puede encontrar el archivo especificado: 'c:\\\\Python312\\\\Scripts\\\\markdown_py.exe' -> 'c:\\\\Python312\\\\Scripts\\\\markdown_py.exe.deleteme'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente instrucción, cuando se corre por primera vez, descarga el conjunto de datos en lo indicado por el parámetro path, relativo a  ~/.keras/datasets).  Como si se hubiera ejecutado Lo siguiente:\n",
    "\n",
    "tf.keras.datasets.mnist.load_data(\n",
    "    path='mnist.npz'\n",
    ")\n",
    "\n",
    "luego separa los datos en un conjunto para entrenamiento y otro para pruebas.\n",
    "\n",
    "Si se ejecuta más de una vez, ya no descarga el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
     ]
    }
   ],
   "source": [
    "(X_entreno, y_entreno), (X_prueba, y_prueba) = tf.keras.datasets.mnist.load_data()\n",
    "assert X_entreno.shape == (60000, 28, 28)\n",
    "assert X_prueba.shape == (10000, 28, 28)\n",
    "assert y_entreno.shape == (60000,)\n",
    "assert y_prueba.shape == (10000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Esta sección es donde pre-procesaremos nuestros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por default, TF2 tiene conjuntos de datos de entrenamiento y de prueba, pero no tiene un conjunto de validación, por lo que debemos dividirlo por nuestra cuenta\n",
    "\n",
    "Lo haremos del mismo tamaño que el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_validacion = y_prueba.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos una variable dedicada para el número de muestras de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_prueba = y_prueba.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalmente preferimos \"normalizar\" nuestros datos en alguna forma para que el resultado sea numéricamente más estable.  En este caso simplemente preferimos tener entradas entre 0 y 1, por lo que definimos una función, que reciba la imagen MNIST.\n",
    "\n",
    "Como los posibles valores de las entradas son entre 0 y 255 (256 posibles tonos de gris), al dividirlos por 255 obtenemos el resultado deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_entreno_normalizado = X_entreno / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, normalizaremos y convertiremos los datos de pruebas en tandas.  Los normalizamos para que tengan la misma magnitud que los datos de entrenamiento y validación.\n",
    "\n",
    "No hay necesidad de \"barajearlo\" ya que no estaremos entrenando con los datos de prueba.  Habra una sola tanda, igual al tamaño de los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_prueba_normalizado = X_prueba / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han \"normalizado\" los datos, podemos proceder a extraer los datos de entrenamiento y de validación.\n",
    "\n",
    "Nuestros datos de validación serán 10000 para ser igual al conjunto de prueba.\n",
    "\n",
    "Finalmente, creamos una tanda con un tamaño de tanda igual al total de muestras de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validacion = X_entreno_normalizado[-num_obs_validacion: , : , : ]\n",
    "y_validacion = y_entreno[-num_obs_validacion:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarmente, los datos de entrenamiento son todos los demás por lo que nos salteamos tantas observaciones como las hay en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_entreno = X_entreno_normalizado[ : X_entreno_normalizado.shape[0] - num_obs_validacion, : , : ]\n",
    "y_entreno = y_entreno[ : y_entreno.shape[0] - num_obs_validacion]\n",
    "num_obs_entreno = y_entreno.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir de Arreglos Numpy a Tensores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barajear y hacer tandas con el conjunto de datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANIO_TANDA = 100\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacer tandas con los conjunto de validación y prueba, no se necesita barajearlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delineamos el modelo\n",
    "\n",
    "Cuando pensamos sobre un algoritmo de aprenzaje profundo, casi siempre solo lo imaginamos.  Asi que esta vez, hagámoslo.  :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tamanio_entrada = 784\n",
    "tamanio_salida = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el mismo ancho para ambas capas escondidas.  No es una necesidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_capa_escondida = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos cómo se verá el modelo\n",
    "\n",
    "La primera capa (la de entrada):  cada observación es de 28x28 píxeles, por lo tanto es un tensor de rango 2.\n",
    "\n",
    "Como aún no hemos aprendido sobre CNNs, no sabemos como alimentar este tipo de entrada a nuestra red, por lo tanto hay que \"aplanar\" las imágenes.  Hay un método conveniente **Flatten** que toma nuestro tensor de 28x28 y lo convierte en  un vector (None), o (784,)...porque 28x28 = 784.  Esto nos permite crear una red de alimentación hacia adelante.\n",
    "\n",
    "    \n",
    "**tf.keras.layers.Dense** básicamente implementa:  output = activation(dot(entrada, peso) + sesgo).  Requiere varios argumentos, pero los más importantes para nosotros son el ancho de la capa escondida y la función de activación.\n",
    "\n",
    "La capa final no es diferente, solo nos aseguramos de activarla con **softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar el optimizador y la función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definimos el optimizador que nos gustaría utilizar, la función de pérdida, y las métricas que nos interesa obtener en cada interacción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Acá es donde entrenamos el modelo que hemos construído\n",
    "\n",
    "Determinamos el número máximo de épocas.\n",
    "\n",
    "Ajustamos el modelo , especificando:\n",
    "\n",
    "* los datos de entrenamiento\n",
    "* el número total de épocas\n",
    "* y los datos de validación que creamos en el formato (entradas, metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.8741 - loss: 0.4405 - val_accuracy: 0.9370 - val_loss: 0.2234\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9409 - loss: 0.1987 - val_accuracy: 0.9549 - val_loss: 0.1601\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9553 - loss: 0.1528 - val_accuracy: 0.9601 - val_loss: 0.1415\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9625 - loss: 0.1257 - val_accuracy: 0.9627 - val_loss: 0.1255\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9683 - loss: 0.1074 - val_accuracy: 0.9627 - val_loss: 0.1266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a69fb0f2c0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar el modelo\n",
    "\n",
    "Como se discutió en clase, luego del entrenamiento (con los datos de entrenamiento), y la validación (con los datos de validación), probamos el potencial de predicción final de nuestro modelo con el conjunto de datos de prueba que el algoritmo NUNCA ha visto antes.\n",
    "\n",
    "Es muy importante reconocer que estar \"jugando\" con los hiperparámetros sobre-ajusta el conjunto de datos de validación.\n",
    "\n",
    "La prueba es la instancia absolutamente final. Nunca debe probarse el modelo antes de haber completamente ajustado el modelo.\n",
    "\n",
    "Si se ajusta el modelo después de hacer la prueba, se empezará a sobre-ajustar el conjunto de datos de prueba, que echaría \"por los suelos\" el propósito original del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9586 - loss: 20.3336 \n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida de prueba: 18.86. Precisión de prueba: 96.38%\n"
     ]
    }
   ],
   "source": [
    "# Si se desea, se puede aplicar un formateo \"bonito\"\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el modelo inicial y los hiperparámetros dados en este notebook, la precisión de prueba final debe ser aproximadamente 97%.\n",
    "\n",
    "Cada vez que se ejecuta el código, se obtiene una precisión diferente debido a la \"barajeada\" de las tandas, los pesos se inicializan en forma diferente, etc.\n",
    "\n",
    "Finalmente, intencionalmente se ha llegado a una solución subóptima, para que puedan tener la oportunidad de mejorarla como ejercicio de laboratorio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9167 - loss: 0.2887 - val_accuracy: 0.9619 - val_loss: 0.1315\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9674 - loss: 0.1087 - val_accuracy: 0.9713 - val_loss: 0.0986\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9769 - loss: 0.0737 - val_accuracy: 0.9713 - val_loss: 0.0900\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9851 - loss: 0.0500 - val_accuracy: 0.9749 - val_loss: 0.0876\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9879 - loss: 0.0380 - val_accuracy: 0.9762 - val_loss: 0.0812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2a69fb79550>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tamanio_capa_escondida = 200\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9703 - loss: 18.2626  \n",
      "Pérdida de prueba: 15.57. Precisión de prueba: 97.46%\n"
     ]
    }
   ],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9219 - loss: 0.2701 - val_accuracy: 0.9654 - val_loss: 0.1192\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9698 - loss: 0.1019 - val_accuracy: 0.9728 - val_loss: 0.0861\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9793 - loss: 0.0667 - val_accuracy: 0.9759 - val_loss: 0.0788\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9847 - loss: 0.0472 - val_accuracy: 0.9757 - val_loss: 0.0779\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.9902 - loss: 0.0315 - val_accuracy: 0.9779 - val_loss: 0.0803\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9721 - loss: 15.4772 \n",
      "Pérdida de prueba: 14.31. Precisión de prueba: 97.57%\n"
     ]
    }
   ],
   "source": [
    "tamanio_capa_escondida = 250\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 3ms/step - accuracy: 0.8717 - loss: 0.4352 - val_accuracy: 0.9403 - val_loss: 0.2116\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9462 - loss: 0.1813 - val_accuracy: 0.9546 - val_loss: 0.1591\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9587 - loss: 0.1357 - val_accuracy: 0.9614 - val_loss: 0.1340\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9658 - loss: 0.1123 - val_accuracy: 0.9650 - val_loss: 0.1288\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9702 - loss: 0.0956 - val_accuracy: 0.9650 - val_loss: 0.1205\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 814us/step - accuracy: 0.9614 - loss: 20.9527\n",
      "Pérdida de prueba: 18.59. Precisión de prueba: 96.61%\n"
     ]
    }
   ],
   "source": [
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 3era capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 3s - 6ms/step - accuracy: 0.9181 - loss: 0.2709 - val_accuracy: 0.9641 - val_loss: 0.1162\n",
      "Epoch 2/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9672 - loss: 0.1098 - val_accuracy: 0.9677 - val_loss: 0.1080\n",
      "Epoch 3/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9764 - loss: 0.0766 - val_accuracy: 0.9706 - val_loss: 0.1062\n",
      "Epoch 4/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9816 - loss: 0.0590 - val_accuracy: 0.9714 - val_loss: 0.1003\n",
      "Epoch 5/5\n",
      "500/500 - 2s - 4ms/step - accuracy: 0.9852 - loss: 0.0475 - val_accuracy: 0.9750 - val_loss: 0.0913\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 15.8529 \n",
      "Pérdida de prueba: 13.74. Precisión de prueba: 97.61%\n"
     ]
    }
   ],
   "source": [
    "tamanio_capa_escondida = 250\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 3era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 4rta capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 5nta capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 - 1s - 2ms/step - accuracy: 0.7707 - loss: 1.0394 - val_accuracy: 0.9042 - val_loss: 0.4194\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9068 - loss: 0.3555 - val_accuracy: 0.9277 - val_loss: 0.2669\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9270 - loss: 0.2599 - val_accuracy: 0.9393 - val_loss: 0.2167\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9391 - loss: 0.2135 - val_accuracy: 0.9483 - val_loss: 0.1893\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9478 - loss: 0.1822 - val_accuracy: 0.9545 - val_loss: 0.1686\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 807us/step - accuracy: 0.9384 - loss: 0.2074\n",
      "Pérdida de prueba: 0.19. Precisión de prueba: 94.44%\n"
     ]
    }
   ],
   "source": [
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='sigmoid'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.8879 - loss: 0.4130 - val_accuracy: 0.9438 - val_loss: 0.1944\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9483 - loss: 0.1729 - val_accuracy: 0.9618 - val_loss: 0.1388\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9622 - loss: 0.1268 - val_accuracy: 0.9655 - val_loss: 0.1179\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9698 - loss: 0.1012 - val_accuracy: 0.9672 - val_loss: 0.1117\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9748 - loss: 0.0848 - val_accuracy: 0.9699 - val_loss: 0.0986\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step - accuracy: 0.9578 - loss: 0.1469\n",
      "Pérdida de prueba: 0.13. Precisión de prueba: 96.27%\n"
     ]
    }
   ],
   "source": [
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='tanh'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "5/5 - 1s - 158ms/step - accuracy: 0.2238 - loss: 2.2078 - val_accuracy: 0.3492 - val_loss: 2.0312\n",
      "Epoch 2/5\n",
      "5/5 - 0s - 35ms/step - accuracy: 0.3925 - loss: 1.9267 - val_accuracy: 0.4927 - val_loss: 1.7454\n",
      "Epoch 3/5\n",
      "5/5 - 0s - 37ms/step - accuracy: 0.5364 - loss: 1.6387 - val_accuracy: 0.6272 - val_loss: 1.4420\n",
      "Epoch 4/5\n",
      "5/5 - 0s - 37ms/step - accuracy: 0.6534 - loss: 1.3472 - val_accuracy: 0.7258 - val_loss: 1.1513\n",
      "Epoch 5/5\n",
      "5/5 - 0s - 36ms/step - accuracy: 0.7374 - loss: 1.0803 - val_accuracy: 0.7917 - val_loss: 0.9043\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7863 - loss: 41.8038\n",
      "Pérdida de prueba: 41.80. Precisión de prueba: 78.63%\n"
     ]
    }
   ],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
    "\n",
    "TAMANIO_TANDA = 10000\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
    "\n",
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 - 38s - 760us/step - accuracy: 0.9212 - loss: 0.2655 - val_accuracy: 0.9483 - val_loss: 0.1846\n",
      "Epoch 2/5\n",
      "50000/50000 - 37s - 744us/step - accuracy: 0.9548 - loss: 0.1592 - val_accuracy: 0.9593 - val_loss: 0.1583\n",
      "Epoch 3/5\n",
      "50000/50000 - 37s - 745us/step - accuracy: 0.9622 - loss: 0.1409 - val_accuracy: 0.9601 - val_loss: 0.1661\n",
      "Epoch 4/5\n",
      "50000/50000 - 38s - 761us/step - accuracy: 0.9655 - loss: 0.1305 - val_accuracy: 0.9597 - val_loss: 0.1859\n",
      "Epoch 5/5\n",
      "50000/50000 - 38s - 755us/step - accuracy: 0.9688 - loss: 0.1239 - val_accuracy: 0.9640 - val_loss: 0.1960\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 479us/step - accuracy: 0.9528 - loss: 63.9141\n",
      "Pérdida de prueba: 55.19. Precisión de prueba: 95.83%\n"
     ]
    }
   ],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
    "\n",
    "TAMANIO_TANDA = 1\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
    "\n",
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 - 31s - 612us/step - accuracy: 0.9126 - loss: 0.2838 - val_accuracy: 0.9452 - val_loss: 0.1803\n",
      "Epoch 2/5\n",
      "50000/50000 - 28s - 553us/step - accuracy: 0.9571 - loss: 0.1442 - val_accuracy: 0.9584 - val_loss: 0.1369\n",
      "Epoch 3/5\n",
      "50000/50000 - 28s - 568us/step - accuracy: 0.9663 - loss: 0.1113 - val_accuracy: 0.9665 - val_loss: 0.1217\n",
      "Epoch 4/5\n",
      "50000/50000 - 29s - 572us/step - accuracy: 0.9709 - loss: 0.0936 - val_accuracy: 0.9638 - val_loss: 0.1267\n",
      "Epoch 5/5\n",
      "50000/50000 - 28s - 568us/step - accuracy: 0.9755 - loss: 0.0808 - val_accuracy: 0.9723 - val_loss: 0.1081\n",
      "\u001b[1m10000/10000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 462us/step - accuracy: 0.9607 - loss: 26.8626\n",
      "Pérdida de prueba: 22.06. Precisión de prueba: 96.63%\n"
     ]
    }
   ],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
    "\n",
    "TAMANIO_TANDA = 1\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
    "\n",
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "modelo.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.6855 - loss: 1.2770 - val_accuracy: 0.8748 - val_loss: 0.5524\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.8833 - loss: 0.4580 - val_accuracy: 0.9089 - val_loss: 0.3479\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9068 - loss: 0.3423 - val_accuracy: 0.9196 - val_loss: 0.2891\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9185 - loss: 0.2958 - val_accuracy: 0.9270 - val_loss: 0.2587\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9247 - loss: 0.2677 - val_accuracy: 0.9318 - val_loss: 0.2384\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 772us/step - accuracy: 0.9139 - loss: 34.4986\n",
      "Pérdida de prueba: 30.74. Precisión de prueba: 92.74%\n"
     ]
    }
   ],
   "source": [
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
    "\n",
    "TAMANIO_TANDA = 100\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size = num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
    "\n",
    "tamanio_capa_escondida = 50\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "\n",
    "learning_rate = 0.0001\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compilar el modelo con el optimizador personalizado\n",
    "modelo.compile(optimizer=optimizador, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "500/500 - 1s - 2ms/step - accuracy: 0.9056 - loss: 0.3160 - val_accuracy: 0.9464 - val_loss: 0.1905\n",
      "Epoch 2/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9406 - loss: 0.2089 - val_accuracy: 0.9476 - val_loss: 0.1940\n",
      "Epoch 3/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9500 - loss: 0.1863 - val_accuracy: 0.9470 - val_loss: 0.2027\n",
      "Epoch 4/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9555 - loss: 0.1628 - val_accuracy: 0.9550 - val_loss: 0.1807\n",
      "Epoch 5/5\n",
      "500/500 - 1s - 1ms/step - accuracy: 0.9578 - loss: 0.1524 - val_accuracy: 0.9365 - val_loss: 0.2193\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 760us/step - accuracy: 0.9211 - loss: 48.9121\n",
      "Pérdida de prueba: 43.08. Precisión de prueba: 93.12%\n"
     ]
    }
   ],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "\n",
    "learning_rate = 0.02\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compilar el modelo con el optimizador personalizado\n",
    "modelo.compile(optimizer=optimizador, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "334/334 - 5s - 16ms/step - accuracy: 0.9061 - loss: 0.3182 - val_accuracy: 0.9473 - val_loss: 0.2066\n",
      "Epoch 2/5\n",
      "334/334 - 3s - 9ms/step - accuracy: 0.9610 - loss: 0.1433 - val_accuracy: 0.9682 - val_loss: 0.1247\n",
      "Epoch 3/5\n",
      "334/334 - 3s - 9ms/step - accuracy: 0.9708 - loss: 0.1110 - val_accuracy: 0.9701 - val_loss: 0.1153\n",
      "Epoch 4/5\n",
      "334/334 - 4s - 11ms/step - accuracy: 0.9777 - loss: 0.0884 - val_accuracy: 0.9722 - val_loss: 0.1118\n",
      "Epoch 5/5\n",
      "334/334 - 5s - 15ms/step - accuracy: 0.9798 - loss: 0.0779 - val_accuracy: 0.9757 - val_loss: 0.1063\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9661 - loss: 17.4262\n",
      "Pérdida de prueba: 15.72. Precisión de prueba: 97.15%\n"
     ]
    }
   ],
   "source": [
    "# Definir los datos\n",
    "datos_entreno = tf.data.Dataset.from_tensor_slices((X_entreno, y_entreno))\n",
    "datos_validacion = tf.data.Dataset.from_tensor_slices((X_validacion, y_validacion))\n",
    "datos_prueba = tf.data.Dataset.from_tensor_slices((X_prueba, y_prueba))\n",
    "\n",
    "TAMANIO_TANDA = 150  # Aumentar el tamaño del lote\n",
    "datos_entreno = datos_entreno.shuffle(buffer_size=num_obs_entreno).batch(TAMANIO_TANDA)\n",
    "datos_validacion = datos_validacion.batch(TAMANIO_TANDA)\n",
    "datos_prueba = datos_prueba.batch(TAMANIO_TANDA)\n",
    "\n",
    "tamanio_capa_escondida = 250\n",
    "\n",
    "\n",
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 3era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 4rta capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 5nta capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 6xta capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 7ima capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])\n",
    "learning_rate = 0.003\n",
    "optimizador = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compilar el modelo con el optimizador personalizado\n",
    "modelo.compile(optimizer=optimizador, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno,\n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = datos_validacion,\n",
    "          verbose = 2)\n",
    "\n",
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
