{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# Cargar datos con las 50,000 palabras más frecuentes\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=150)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Obtener el índice de palabras del dataset de IMDB\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "# Invertir el índice para que puedas decodificar las secuencias numéricas\n",
    "inverted_word_index = dict(\n",
    "    (i + 3, word) for (word, i) in word_index.items()\n",
    ")\n",
    "inverted_word_index[1] = \"[START]\"\n",
    "inverted_word_index[2] = \"[OOV]\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir listas básicas de palabras positivas y negativas\n",
    "positive_words = set([\"good\", \"great\", \"excellent\", \"amazing\", \"awesome\", \"fantastic\"])\n",
    "negative_words = set([\"bad\", \"terrible\", \"horrible\", \"worst\", \"awful\"])\n",
    "\n",
    "def extract_features(sequences, word_index):\n",
    "    features = []\n",
    "    for sequence in sequences:\n",
    "        decoded_review = [inverted_word_index.get(i, \"\") for i in sequence]\n",
    "        length = len(sequence)  # Longitud de la crítica\n",
    "        positive_count = sum(1 for word in decoded_review if word in positive_words)\n",
    "        negative_count = sum(1 for word in decoded_review if word in negative_words)\n",
    "        total_words = len(decoded_review)\n",
    "        if total_words > 0:\n",
    "            pos_neg_ratio = (positive_count - negative_count) / total_words\n",
    "        else:\n",
    "            pos_neg_ratio = 0\n",
    "        features.append([length, pos_neg_ratio])\n",
    "    return np.array(features)\n",
    "\n",
    "# Extraer características para el set de entrenamiento y prueba\n",
    "train_features = extract_features(X_train, word_index)\n",
    "test_features = extract_features(X_test, word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_features_scaled = scaler.fit_transform(train_features)\n",
    "test_features_scaled = scaler.transform(test_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Entrada de secuencias de longitud 150\n",
    "input_seq = Input(shape=(150,))\n",
    "x = Embedding(50000, 128)(input_seq)\n",
    "x = LSTM(128, return_sequences=True)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = LSTM(64, return_sequences=False)(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Entrada de características adicionales (2 características)\n",
    "input_features = Input(shape=(2,))\n",
    "\n",
    "# Concatenar ambas entradas\n",
    "concat = Concatenate()([x, input_features])\n",
    "\n",
    "# Capas densas adicionales\n",
    "x = Dense(64, activation='relu')(concat)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Definir el modelo\n",
    "model = Model([input_seq, input_features], output)\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "782/782 - 202s - 259ms/step - accuracy: 0.7449 - loss: 0.5151 - val_accuracy: 0.8049 - val_loss: 0.4288\n",
      "Epoch 2/15\n",
      "782/782 - 204s - 261ms/step - accuracy: 0.8656 - loss: 0.3261 - val_accuracy: 0.7731 - val_loss: 0.4906\n",
      "Epoch 3/15\n",
      "782/782 - 195s - 249ms/step - accuracy: 0.9154 - loss: 0.2283 - val_accuracy: 0.8565 - val_loss: 0.3430\n",
      "Epoch 4/15\n",
      "782/782 - 288s - 369ms/step - accuracy: 0.9578 - loss: 0.1271 - val_accuracy: 0.8475 - val_loss: 0.4049\n",
      "Epoch 5/15\n",
      "782/782 - 1109s - 1s/step - accuracy: 0.9762 - loss: 0.0743 - val_accuracy: 0.8354 - val_loss: 0.5464\n",
      "Epoch 6/15\n",
      "782/782 - 189s - 241ms/step - accuracy: 0.9832 - loss: 0.0543 - val_accuracy: 0.8440 - val_loss: 0.6007\n",
      "Epoch 7/15\n",
      "782/782 - 1366s - 2s/step - accuracy: 0.9919 - loss: 0.0273 - val_accuracy: 0.8447 - val_loss: 0.6779\n",
      "Epoch 8/15\n",
      "782/782 - 189s - 242ms/step - accuracy: 0.9927 - loss: 0.0240 - val_accuracy: 0.8400 - val_loss: 0.6954\n",
      "Epoch 9/15\n",
      "782/782 - 693s - 886ms/step - accuracy: 0.9950 - loss: 0.0156 - val_accuracy: 0.8429 - val_loss: 0.5532\n",
      "Epoch 10/15\n",
      "782/782 - 138s - 177ms/step - accuracy: 0.9952 - loss: 0.0155 - val_accuracy: 0.8329 - val_loss: 0.7074\n",
      "Epoch 11/15\n",
      "782/782 - 130s - 167ms/step - accuracy: 0.9960 - loss: 0.0136 - val_accuracy: 0.8397 - val_loss: 0.7408\n",
      "Epoch 12/15\n",
      "782/782 - 123s - 158ms/step - accuracy: 0.9967 - loss: 0.0110 - val_accuracy: 0.8430 - val_loss: 0.8283\n",
      "Epoch 13/15\n",
      "782/782 - 119s - 153ms/step - accuracy: 0.9974 - loss: 0.0084 - val_accuracy: 0.8417 - val_loss: 1.0947\n",
      "Epoch 14/15\n",
      "782/782 - 931s - 1s/step - accuracy: 0.9976 - loss: 0.0079 - val_accuracy: 0.8297 - val_loss: 1.0295\n",
      "Epoch 15/15\n",
      "782/782 - 125s - 160ms/step - accuracy: 0.9974 - loss: 0.0080 - val_accuracy: 0.8395 - val_loss: 0.8289\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x314d243d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo con la nueva longitud de secuencia y características\n",
    "model.fit([X_train, train_features_scaled], y_train, \n",
    "          batch_size=32, \n",
    "          epochs=15, \n",
    "          validation_data=([X_test, test_features_scaled], y_test), \n",
    "          verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 29s - 37ms/step - accuracy: 0.8395 - loss: 0.8289\n",
      "Pérdida de la prueba: 0.8288770914077759\n",
      "Exactitud de la prueba: 0.8394799828529358\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "loss, accuracy = model.evaluate([X_test, test_features_scaled], y_test, batch_size=32, verbose=2)\n",
    "print('Pérdida de la prueba:', loss)\n",
    "print('Exactitud de la prueba:', accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
